{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import argparse\n",
                "import os\n",
                "import pathlib\n",
                "import pprint\n",
                "import sqlite3\n",
                "from contextlib import closing\n",
                "from functools import reduce\n",
                "\n",
                "import duckdb\n",
                "import pandas as pd\n",
                "\n",
                "try:\n",
                "    cfg = get_ipython().config\n",
                "    in_notebook = True\n",
                "except NameError:\n",
                "    in_notebook = False\n",
                "\n",
                "    # Get the current working directory\n",
                "cwd = pathlib.Path.cwd()\n",
                "\n",
                "if (cwd / \".git\").is_dir():\n",
                "    root_dir = cwd\n",
                "\n",
                "else:\n",
                "    root_dir = None\n",
                "    for parent in cwd.parents:\n",
                "        if (parent / \".git\").is_dir():\n",
                "            root_dir = parent\n",
                "            break\n",
                "\n",
                "# Check if a Git root directory was found\n",
                "if root_dir is None:\n",
                "    raise FileNotFoundError(\"No Git root directory found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "105 parquet files found\n"
                    ]
                }
            ],
            "source": [
                "if not in_notebook:\n",
                "    argparser = argparse.ArgumentParser()\n",
                "    argparser.add_argument(\n",
                "        \"--well_fov\",\n",
                "        type=str,\n",
                "        required=True,\n",
                "        help=\"Well and field of view to process, e.g. 'A01_1'\",\n",
                "    )\n",
                "    argparser.add_argument(\n",
                "        \"--patient\",\n",
                "        type=str,\n",
                "        required=True,\n",
                "        help=\"Patient ID to process, e.g. 'P01'\",\n",
                "    )\n",
                "    args = argparser.parse_args()\n",
                "    well_fov = args.well_fov\n",
                "    patient = args.patient\n",
                "else:\n",
                "    well_fov = \"C4-2\"\n",
                "    patient = \"NF0014\"\n",
                "\n",
                "\n",
                "result_path = pathlib.Path(\n",
                "    f\"{root_dir}/data/{patient}/extracted_features/{well_fov}\"\n",
                ").resolve(strict=True)\n",
                "database_path = pathlib.Path(\n",
                "    f\"{root_dir}/data/{patient}/converted_profiles/{well_fov}\"\n",
                ").resolve()\n",
                "database_path.mkdir(parents=True, exist_ok=True)\n",
                "# create the sqlite database\n",
                "sqlite_path = database_path / f\"{well_fov}.duckdb\"\n",
                "\n",
                "\n",
                "# get a list of all parquets in the directory recursively\n",
                "parquet_files = list(result_path.rglob(\"*.parquet\"))\n",
                "parquet_files.sort()\n",
                "print(len(parquet_files), \"parquet files found\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create the nested dictionary to hold the feature types and compartments\n",
                "feature_types = [\n",
                "    \"AreaSize_Shape\",\n",
                "    \"Colocalization\",\n",
                "    \"Intensity\",\n",
                "    \"Granularity\",\n",
                "    \"Neighbor\",\n",
                "    \"Texture\",\n",
                "]\n",
                "compartments = [\"Organoid\", \"Nuclei\", \"Cell\", \"Cytoplasm\"]\n",
                "\n",
                "feature_types_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
                "# copy the feature types dictionary to another blank dictionary that will hold the parquet files\n",
                "\n",
                "merged_df_dict = {cmp: {ft: [] for ft in feature_types} for cmp in compartments}\n",
                "\n",
                "\n",
                "for file in parquet_files:\n",
                "    for compartment in feature_types_dict.keys():\n",
                "        for feature_type in feature_types_dict[compartment].keys():\n",
                "            if compartment in str(file) and feature_type in str(file):\n",
                "                feature_types_dict[compartment][feature_type].append(file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "for compartment in feature_types_dict.keys():\n",
                "    for feature_type in feature_types_dict[compartment].keys():\n",
                "        if len(feature_types_dict[compartment][feature_type]) > 0:\n",
                "            for file in feature_types_dict[compartment][feature_type]:\n",
                "                # check if the file exists\n",
                "                if not file.exists():\n",
                "                    if (\n",
                "                        \"neighbor\" in file.name.lower()\n",
                "                        and \"nuclei\" not in file.name.lower()\n",
                "                    ):\n",
                "                        print(f\"File {file} does not exist\")\n",
                "                        continue\n",
                "                # check if the file is a parquet file\n",
                "                if not file.name.endswith(\".parquet\"):\n",
                "                    print(f\"File {file} is not a parquet file\")\n",
                "                    continue\n",
                "                # read the parquet files\n",
                "                try:\n",
                "                    df = duckdb.read_parquet(str(file)).to_df()\n",
                "                except Exception as e:\n",
                "                    print(\n",
                "                        f\"Error reading {feature_types_dict[compartment][feature_type]}: {e}\"\n",
                "                    )\n",
                "\n",
                "                # add the dataframe to the dictionary\n",
                "                merged_df_dict[compartment][feature_type].append(df)\n",
                "        else:\n",
                "            if (\n",
                "                \"neighbor\" in feature_type.lower()\n",
                "                and \"nuclei\" not in compartment.lower()\n",
                "            ):\n",
                "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
                "            else:\n",
                "                print(\n",
                "                    f\"No files found for {compartment} {feature_type}. Please check the directory.\"\n",
                "                )\n",
                "                merged_df_dict[compartment][feature_type].append(pd.DataFrame())\n",
                "                for channel_df in merged_df_dict[compartment][feature_type]:\n",
                "                    if channel_df.empty:\n",
                "                        continue\n",
                "                    # check if the dataframe has the required columns\n",
                "                    if (\n",
                "                        \"object_id\" not in channel_df.columns\n",
                "                        or \"image_set\" not in channel_df.columns\n",
                "                    ):\n",
                "                        print(\n",
                "                            f\"Dataframe {channel_df} does not have the required columns\"\n",
                "                        )\n",
                "                        continue\n",
                "                    # check if the dataframe is empty\n",
                "                    if channel_df.empty:\n",
                "                        continue"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_types = [\n",
                "    \"AreaSize_Shape\",\n",
                "    \"Colocalization\",\n",
                "    \"Intensity\",\n",
                "    \"Granularity\",\n",
                "    \"Neighbor\",\n",
                "    \"Texture\",\n",
                "]\n",
                "compartments = [\"Organoid\", \"Nuclei\", \"Cell\", \"Cytoplasm\"]\n",
                "\n",
                "final_df_dict = {\n",
                "    cmp: {ft: pd.DataFrame() for ft in feature_types} for cmp in compartments\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "for compartment in merged_df_dict.keys():\n",
                "    for feature_type in merged_df_dict[compartment].keys():\n",
                "        for df in merged_df_dict[compartment][feature_type]:\n",
                "            if df.empty:\n",
                "                continue\n",
                "            df.drop(columns=[\"__index_level_0__\"], inplace=True, errors=\"ignore\")\n",
                "            # if \"Texture\" not in feature_type:\n",
                "            final_df_dict[compartment][feature_type] = reduce(\n",
                "                lambda left, right: pd.merge(\n",
                "                    left, right, how=\"left\", on=[\"object_id\", \"image_set\"]\n",
                "                ),\n",
                "                merged_df_dict[compartment][feature_type],\n",
                "            )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "merged_df = pd.DataFrame(\n",
                "    {\n",
                "        \"object_id\": [],\n",
                "        \"image_set\": [],\n",
                "    }\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "compartment_merged_dict = {\n",
                "    \"Organoid\": pd.DataFrame(),\n",
                "    \"Cell\": pd.DataFrame(),\n",
                "    \"Nuclei\": pd.DataFrame(),\n",
                "    \"Cytoplasm\": pd.DataFrame(),\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing compartment: Organoid\n",
                        "Skipping Organoid Neighbor as it is not applicable for this compartment.\n",
                        "Processing compartment: Nuclei\n",
                        "Processing compartment: Cell\n",
                        "Skipping Cell Neighbor as it is not applicable for this compartment.\n",
                        "Processing compartment: Cytoplasm\n",
                        "Skipping Cytoplasm Neighbor as it is not applicable for this compartment.\n"
                    ]
                }
            ],
            "source": [
                "for compartment in final_df_dict.keys():\n",
                "    print(f\"Processing compartment: {compartment}\")\n",
                "    for feature_type in final_df_dict[compartment].keys():\n",
                "        if compartment != \"Nuclei\" and feature_type == \"Neighbor\":\n",
                "            print(\n",
                "                f\"Skipping {compartment} {feature_type} as it is not applicable for this compartment.\"\n",
                "            )\n",
                "            continue\n",
                "        if compartment_merged_dict[compartment].empty:\n",
                "            compartment_merged_dict[compartment] = final_df_dict[compartment][\n",
                "                feature_type\n",
                "            ].copy()\n",
                "        else:\n",
                "            compartment_merged_dict[compartment] = pd.merge(\n",
                "                compartment_merged_dict[compartment],\n",
                "                final_df_dict[compartment][feature_type],\n",
                "                on=[\"object_id\", \"image_set\"],\n",
                "                how=\"outer\",\n",
                "            )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "with duckdb.connect(sqlite_path) as cx:\n",
                "    for compartment, df in compartment_merged_dict.items():\n",
                "        cx.register(\"temp_df\", df)\n",
                "        cx.execute(f\"CREATE OR REPLACE TABLE {compartment} AS SELECT * FROM temp_df\")\n",
                "        cx.unregister(\"temp_df\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "nf1_image_based_profiling_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
