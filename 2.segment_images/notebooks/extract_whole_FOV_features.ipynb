{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f662cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import tifffile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from arg_parsing_utils import check_for_missing_args, parse_args\n",
    "from file_reading import *\n",
    "from notebook_init_utils import bandicoot_check, init_notebook\n",
    "from sammed3d_featurizer import call_whole_image_sammed3d_pipeline\n",
    "from torchvision import transforms as v2\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69c0c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# get starting memory (cpu)\n",
    "start_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532ad014",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir, in_notebook = init_notebook()\n",
    "\n",
    "image_base_dir = bandicoot_check(\n",
    "    pathlib.Path(os.path.expanduser(\"~/mnt/bandicoot\")).resolve(), root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa86063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a notebook\n"
     ]
    }
   ],
   "source": [
    "if not in_notebook:\n",
    "    args = parse_args()\n",
    "    well_fov = args[\"well_fov\"]\n",
    "    patient = args[\"patient\"]\n",
    "    input_subparent_name = args[\"input_subparent_name\"]\n",
    "    check_for_missing_args(\n",
    "        well_fov=well_fov,\n",
    "        patient=patient,\n",
    "        input_subparent_name=input_subparent_name,\n",
    "    )\n",
    "else:\n",
    "    print(\"Running in a notebook\")\n",
    "    patient = \"NF0014_T1\"\n",
    "    well_fov = \"C4-2\"\n",
    "    input_subparent_name = \"zstack_images\"\n",
    "\n",
    "\n",
    "input_dir = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/{input_subparent_name}/{well_fov}\"\n",
    ").resolve(strict=True)\n",
    "# save path\n",
    "feature_save_path = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/whole_image_features/{well_fov}_whole_image_features.parquet\"\n",
    ").resolve()\n",
    "feature_save_path.parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad89ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features already exist at /home/lippincm/mnt/bandicoot/NF1_organoid_data/data/NF0014_T1/whole_image_features/C4-2_whole_image_features.parquet, skipping...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lippincm/miniforge3/envs/GFF_segmentation_nuclei/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if feature_save_path.exists():\n",
    "    print(f\"Features already exist at {feature_save_path}, skipping...\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise Injector transformation\n",
    "\n",
    "\n",
    "class SaturationNoiseInjector(nn.Module):\n",
    "    def __init__(self, low=200, high=255):\n",
    "        super().__init__()\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        channel = x[0].clone()\n",
    "        noise = torch.empty_like(channel).uniform_(self.low, self.high)\n",
    "        mask = (channel == 255).float()\n",
    "        noise_masked = noise * mask\n",
    "        channel[channel == 255] = 0\n",
    "        channel = channel + noise_masked\n",
    "        x[0] = channel\n",
    "        return x\n",
    "\n",
    "\n",
    "# Self Normalize transformation\n",
    "class PerImageNormalize(nn.Module):\n",
    "    def __init__(self, eps=1e-7):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.instance_norm = nn.InstanceNorm2d(\n",
    "            num_features=1,\n",
    "            affine=False,\n",
    "            track_running_stats=False,\n",
    "            eps=self.eps,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.instance_norm(x)\n",
    "        if x.shape[0] == 1:\n",
    "            x = x.squeeze(0)\n",
    "        return x\n",
    "\n",
    "\n",
    "def featurize_2D_image_w_chami75(\n",
    "    image_tensor: torch.Tensor, model: torch.nn.Module, device: torch.device\n",
    "):\n",
    "    # Bag of Channels (BoC) - process each channel independently\n",
    "    with torch.no_grad():\n",
    "        batch_feat = []\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        for c in range(image_tensor.shape[1]):\n",
    "            # Extract single channel: (N, C, H, W) -> (N, 1, H, W)\n",
    "            # where:\n",
    "            # N is batch size (1 in this case),\n",
    "            # C is number of channels,\n",
    "            # H and W are Y and X dimensions\n",
    "            single_channel = image_tensor[:, c, :, :].unsqueeze(1)\n",
    "\n",
    "            # Apply transforms\n",
    "            single_channel = transform(single_channel.squeeze(1)).unsqueeze(1)\n",
    "\n",
    "            # Extract features\n",
    "            output = model.forward_features(single_channel)\n",
    "            feat_temp = output[\"x_norm_clstoken\"].cpu().detach().numpy()\n",
    "            batch_feat.append(feat_temp)\n",
    "    return batch_feat[0]\n",
    "\n",
    "\n",
    "# load models\n",
    "sam3dmed_checkpoint_url = (\n",
    "    \"https://huggingface.co/blueyo0/SAM-Med3D/resolve/main/sam_med3d_turbo.pth\"\n",
    ")\n",
    "sam3dmed_checkpoint_path = pathlib.Path(\"../models/sam-med3d-turbo.pth\").resolve()\n",
    "if not sam3dmed_checkpoint_path.exists():\n",
    "    sam3dmed_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(sam3dmed_checkpoint_url, str(sam3dmed_checkpoint_path))\n",
    "\n",
    "# Load model\n",
    "device = \"cuda\"\n",
    "model = AutoModel.from_pretrained(\"CaicedoLab/MorphEm\", trust_remote_code=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "# Define transforms\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        SaturationNoiseInjector(),\n",
    "        PerImageNormalize(),\n",
    "        v2.Resize(size=(224, 224), antialias=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ef71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all well fovs for this patient\n",
    "\n",
    "images_to_process = {\"patient\": [], \"well_fov\": [], \"image\": [], \"channel\": []}\n",
    "\n",
    "images_to_load = [x for x in input_dir.glob(\"*.tif\")]\n",
    "for image_file in images_to_load:\n",
    "    image = read_zstack_image(image_file)\n",
    "    # load the middle slice\n",
    "    mid_slice = image.shape[0] // 2\n",
    "    image_mid = image[mid_slice, :, :]\n",
    "    images_to_process[\"patient\"].append(patient)\n",
    "    images_to_process[\"well_fov\"].append(well_fov)\n",
    "    images_to_process[\"image\"].append(image_mid)\n",
    "    images_to_process[\"channel\"].append(f\"{image_file.stem.split('_')[1]}\")\n",
    "\n",
    "# Convert list of 2D images (H, W) to tensor (B, C, H, W)\n",
    "# where B is batch size (number of images),\n",
    "# C is number of channels (1 in this case),\n",
    "# H and W are Y and X dimensions\n",
    "# Stack images and add channel dimension\n",
    "images = torch.stack(\n",
    "    [torch.tensor(img, dtype=torch.float32) for img in images_to_process[\"image\"]]\n",
    ")\n",
    "# images is now (B, H, W), add channel dimension -> (B, 1, H, W)\n",
    "images = images.unsqueeze(1)\n",
    "# Replicate channel 3 times to get (B, 3, H, W)\n",
    "images = images.repeat(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3597be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    \"patient\": [],\n",
    "    \"well_fov\": [],\n",
    "    \"feature_name\": [],\n",
    "    \"feature_value\": [],\n",
    "}\n",
    "\n",
    "for image_index in range(images.shape[0]):\n",
    "    channel_id = images_to_process[\"channel\"][image_index]\n",
    "\n",
    "    image = images[image_index].cpu().numpy()\n",
    "    output_dict = call_whole_image_sammed3d_pipeline(\n",
    "        image=image,\n",
    "        SAMMed3D_model_path=str(sam3dmed_checkpoint_path),\n",
    "        feature_type=\"cls\",\n",
    "    )\n",
    "    feature_dict[\"patient\"].extend([f\"{patient}\"] * len(output_dict[\"feature_name\"]))\n",
    "    feature_dict[\"well_fov\"].extend([f\"{well_fov}\"] * len(output_dict[\"feature_name\"]))\n",
    "    feature_dict[\"feature_name\"].extend(\n",
    "        f\"{channel_id}_{feature_name}\" for feature_name in output_dict[\"feature_name\"]\n",
    "    )\n",
    "    feature_dict[\"feature_value\"].extend(output_dict[\"value\"])\n",
    "\n",
    "    batch_feat = featurize_2D_image_w_chami75(images, model, device)\n",
    "    for f_idx in range(batch_feat.shape[1]):\n",
    "        feature_name = f\"{channel_id}_CHAMI75_feature_{f_idx}\"\n",
    "        feature_value = batch_feat[image_index, f_idx]\n",
    "        feature_dict[\"patient\"].extend([f\"{patient}\"])\n",
    "        feature_dict[\"well_fov\"].extend([f\"{well_fov}\"])\n",
    "        feature_dict[\"feature_name\"].append(feature_name)\n",
    "        feature_dict[\"feature_value\"].append(feature_value)\n",
    "\n",
    "df = pd.DataFrame(feature_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.pivot_table(\n",
    "        index=[\"patient\", \"well_fov\"], columns=\"feature_name\", values=\"feature_value\"\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename_axis(None, axis=1)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\n",
    "    feature_save_path,\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ec37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()\n",
    "# get starting memory (cpu)\n",
    "end_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(f\"Memory used: {end_mem - start_mem} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GFF_segmentation_nuclei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
