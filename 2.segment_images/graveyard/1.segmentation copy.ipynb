{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edf81bd",
   "metadata": {},
   "source": [
    "This runs all segmentation operations in one place.\n",
    "The idea is that this should be faster and easier to envoke as we only have to load the image data once instead of N times (~10).\n",
    "Running each individual task as its own script is modular but requires overhead to load the data each time.\n",
    "Currently it takes about 15 minutes to complete a single organoid's segmentation for all compartments... (~50,1500,1500) (Z,Y,X) dimensional image. \n",
    "Let us see how long this takes!\n",
    "\n",
    "No we are at ~8 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7512471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import scipy\n",
    "import tifffile\n",
    "import torch\n",
    "from arg_parsing_utils import check_for_missing_args, parse_args\n",
    "from cellpose import models\n",
    "from file_reading import *\n",
    "from file_reading import read_zstack_image\n",
    "from general_segmentation_utils import *\n",
    "from notebook_init_utils import bandicoot_check, init_notebook\n",
    "from organoid_segmentation import *\n",
    "from segmentation_decoupling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# get starting memory (cpu)\n",
    "start_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dab7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir, in_notebook = init_notebook()\n",
    "\n",
    "image_base_dir = bandicoot_check(\n",
    "    pathlib.Path(os.path.expanduser(\"~/mnt/bandicoot\")).resolve(), root_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ed9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not in_notebook:\n",
    "    args = parse_args()\n",
    "    clip_limit = args[\"clip_limit\"]\n",
    "    well_fov = args[\"well_fov\"]\n",
    "    patient = args[\"patient\"]\n",
    "    input_subparent_name = args[\"input_subparent_name\"]\n",
    "    mask_subparent_name = args[\"mask_subparent_name\"]\n",
    "    check_for_missing_args(\n",
    "        well_fov=well_fov,\n",
    "        patient=patient,\n",
    "        clip_limit=clip_limit,\n",
    "        input_subparent_name=input_subparent_name,\n",
    "        mask_subparent_name=mask_subparent_name,\n",
    "    )\n",
    "else:\n",
    "    print(\"Running in a notebook\")\n",
    "    patient = \"NF0014_T1\"\n",
    "    well_fov = \"C4-2\"\n",
    "    clip_limit = 0.01\n",
    "    input_subparent_name = \"zstack_images\"\n",
    "    mask_subparent_name = \"segmentation_masks\"\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "input_dir = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/{input_subparent_name}/{well_fov}\"\n",
    ").resolve(strict=True)\n",
    "mask_path = pathlib.Path(\n",
    "    f\"{image_base_dir}/data/{patient}/{mask_subparent_name}/{well_fov}\"\n",
    ").resolve()\n",
    "mask_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ebc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the morphology of the organoid from json file\n",
    "# json_file_path = pathlib.Path(\"./organoid_image_labels.json\").resolve(strict=True)\n",
    "# with open(json_file_path, \"r\") as f:\n",
    "#     organoid_image_labels = json.load(f)\n",
    "# organoid_image_labels_df = pd.DataFrame(organoid_image_labels)\n",
    "# # look up the morphology for this well_fov\n",
    "# morphology = organoid_image_labels_df.loc[\n",
    "#     organoid_image_labels_df[\"well_fov\"] == well_fov, \"label\"\n",
    "# ].values[0]\n",
    "morphology = \"globular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71418",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_dict = read_in_channels(\n",
    "    find_files_available(input_dir),\n",
    "    channel_dict={\n",
    "        \"nuclei\": \"405\",\n",
    "        \"cyto1\": \"488\",\n",
    "        \"cyto2\": \"555\",\n",
    "        \"cyto3\": \"640\",\n",
    "        \"brightfield\": \"TRANS\",\n",
    "    },\n",
    "    channels_to_read=[\"cyto2\"],\n",
    ")\n",
    "cyto2_raw = return_dict[\"cyto2\"]\n",
    "del return_dict\n",
    "nuclei_mask_output = pathlib.Path(f\"{mask_path}/nuclei_mask.tiff\")\n",
    "nuclei_mask = read_zstack_image(nuclei_mask_output)\n",
    "# run clip_limit here\n",
    "cyto2 = skimage.exposure.equalize_adapthist(\n",
    "    cyto2_raw, clip_limit=clip_limit, kernel_size=None\n",
    ")\n",
    "del cyto2_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ecf83",
   "metadata": {},
   "source": [
    "## Organoid segmentation\n",
    "Commented out as this segmentation is now derived from cell segmentation.\n",
    "This is done in a separate notebook `1a.organoid_segmentation_derived_from_cell.ipynb`.\n",
    "This code is effectively deprecated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b225568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cyto2_image_shape = cyto2.shape\n",
    "# # commenting out optimization for time being\n",
    "# # uncomment to run optimization\n",
    "# # butterworth_grid_optimization(two_point_five_D_sliding_window, return_plot=False)\n",
    "\n",
    "# filtered_cyto2 = apply_butterworth_filter(\n",
    "#     sliding_window_two_point_five_D(cyto2, window_size=window_size),  # cyto\n",
    "#     cutoff_frequency_ratio=0.05,\n",
    "#     order=1,\n",
    "#     high_pass=False,\n",
    "#     squared_butterworth=True,\n",
    "# )\n",
    "# model = models.CellposeModel(\n",
    "#     gpu=[True if torch.cuda.is_available() else False][0],\n",
    "#     model_type=\"cyto3\",  # CP3\n",
    "# )\n",
    "# output_dict = {\n",
    "#     \"slice\": [],\n",
    "#     \"labels\": [],\n",
    "#     \"details\": [],\n",
    "# }\n",
    "# for slice in tqdm.tqdm(range(filtered_cyto2.shape[0])):\n",
    "#     labels, details, _ = segment_with_diameter(\n",
    "#         filtered_cyto2[slice],\n",
    "#         model=model,\n",
    "#         diameter=750,\n",
    "#         z_axis=0,\n",
    "#         channels=[1, 0],\n",
    "#         min_diameter=200,  # default 200\n",
    "#         diameter_step=200,  # default 200\n",
    "#     )\n",
    "#     output_dict[\"slice\"].append(slice)\n",
    "#     output_dict[\"labels\"].append(labels)\n",
    "#     output_dict[\"details\"].append(details)\n",
    "\n",
    "# del filtered_cyto2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71106b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organoid_masks = np.array(\n",
    "#     list(\n",
    "#         decouple_masks(\n",
    "#             reverse_sliding_window_max_projection(\n",
    "#                 output_dict,\n",
    "#                 window_size=window_size,\n",
    "#                 original_z_slice_count=cyto2_image_shape[0],\n",
    "#             ),\n",
    "#             original_img_shape=cyto2_image_shape,\n",
    "#             distance_threshold=40,\n",
    "#         ).values()\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd899950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate the coordinates dataframe for reconstruction\n",
    "# coordinates_df = generate_coordinates_for_reconstruction(organoid_masks)\n",
    "# # generate distance pairs dataframe\n",
    "# df = generate_distance_pairs(coordinates_df, x_y_vector_radius_max_constraint=20)\n",
    "# # create and solve graph to get longest paths\n",
    "# longest_paths = solve_graph(graph_creation(df))\n",
    "# # collapse labels based on longest paths and reassign labels in organoid masks\n",
    "# image = reassign_labels(organoid_masks, collapse_labels(coordinates_df, longest_paths))\n",
    "# # refine the organoid masks\n",
    "# organoid_mask = run_post_hoc_refinement(\n",
    "#     mask_image=image,\n",
    "#     sliding_window_context=3,\n",
    "# )\n",
    "# # clean up and send to gc\n",
    "# del image, coordinates_df, df, longest_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5473d",
   "metadata": {},
   "source": [
    "## Segment the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e3678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_cells_with_3D_watershed(\n",
    "    cyto_signal: np.ndarray, nuclei_mask: np.ndarray, masked_region: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # gaussian filter to smooth the image\n",
    "    # cell_signal_image = skimage.filters.gaussian(cyto_signal, sigma=1.0)\n",
    "    # # scale the pixels to max 255\n",
    "    # nuclei_mask = (nuclei_mask / nuclei_mask.max() * 255).astype(np.uint8)\n",
    "    # # generate the elevation map using the Sobel filter\n",
    "    # elevation_map = sobel(cell_signal_image)\n",
    "\n",
    "    # set up seeded watersheding where the nuclei masks are used as seeds\n",
    "    # note: the cytoplasm is used as the signal for this.\n",
    "    labels = skimage.segmentation.watershed(\n",
    "        image=cyto_signal,\n",
    "        markers=nuclei_mask,\n",
    "        mask=masked_region,\n",
    "        connectivity=0,\n",
    "        compactness=0,\n",
    "        watershed_line=True,\n",
    "    )\n",
    "\n",
    "    # change the largest label (by area) to 0\n",
    "    # cleans up the output and sets the background properly\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    largest_label = unique[np.argmax(counts)]\n",
    "    labels[labels == largest_label] = 0\n",
    "    # cell_mask = labels.copy()\n",
    "    # cell_mask = run_post_hoc_refinement(\n",
    "    #     mask_image=cell_mask,\n",
    "    #     sliding_window_context=3,\n",
    "    # )\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64c2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_option = \"magma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1f1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation_map_1 = skimage.filters.butterworth(\n",
    "#     cyto2,\n",
    "#     cutoff_frequency_ratio=0.08,\n",
    "#     order=2,\n",
    "#     high_pass=False,\n",
    "#     squared_butterworth=False,\n",
    "# )\n",
    "# threshold = skimage.filters.threshold_otsu(elevation_map_1)\n",
    "# cyto2_threshold_signal = cyto2 > threshold\n",
    "# # remove small objects\n",
    "# for z in range(cyto2_threshold_signal.shape[0]):\n",
    "#     cyto2_threshold_signal[z] = skimage.morphology.remove_small_objects(\n",
    "#         cyto2_threshold_signal[z].astype(bool), min_size=500\n",
    "#     )\n",
    "\n",
    "# # fill holes\n",
    "# for z in range(cyto2_threshold_signal.shape[0]):\n",
    "#     cyto2_threshold_signal[z] = scipy.ndimage.binary_fill_holes(\n",
    "#         cyto2_threshold_signal[z]\n",
    "#     )\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(cyto2[cyto2.shape[0] // 2], cmap=cmap_option)\n",
    "# plt.title(\"Cyto2 Image\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(elevation_map_1[cyto2.shape[0] // 2], cmap=cmap_option)\n",
    "# plt.title(\"Low-pass Butterworth filter\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(cyto2_threshold_signal[cyto2.shape[0] // 2],cmap=\"nipy_spectral\")\n",
    "# plt.title(\"Otsu Threshold\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da4ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling=[1, 0.1, 0.1]\n",
    "\n",
    "# cyto_signal = cyto2.copy()\n",
    "# cyto_signal[cyto2_threshold_signal == 0] = 0\n",
    "# markers = nuclei_mask\n",
    "# masked_region = cyto2_threshold_signal\n",
    "\n",
    "# cyto_signal = skimage.filters.gaussian(cyto_signal, sigma=1.0)\n",
    "# cyto_signal = skimage.filters.sobel(cyto_signal)\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(nuclei_mask[8], cmap=\"nipy_spectral\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Nuclei Mask Dilated\")\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(cyto_signal[8], cmap=\"inferno\")\n",
    "# # plt.title(cyto_signal_name)\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(masked_region[8], cmap=\"nipy_spectral\")\n",
    "# plt.title(\"Masked Region\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05583f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import skimage.transform as skt\n",
    "# import numpy as np\n",
    "\n",
    "# def resample_isotropic(vol, spacing, order=1):\n",
    "#     # spacing = (z, y, x) in microns (or any consistent units)\n",
    "#     z, y, x = spacing\n",
    "#     scale = (z / x, 1.0, 1.0)  # target isotropic using x as reference\n",
    "#     out_shape = (\n",
    "#         int(vol.shape[0] * scale[0]),\n",
    "#         vol.shape[1],\n",
    "#         vol.shape[2],\n",
    "#     )\n",
    "#     return skt.resize(vol, out_shape, order=order, preserve_range=True, anti_aliasing=(order>0)).astype(vol.dtype)\n",
    "\n",
    "# # Example\n",
    "# spacing = (1.0, 0.1, 0.1)  # z, y, x\n",
    "# nuclei_iso = resample_isotropic(nuclei_mask, spacing, order=0)   # labels -> order=0\n",
    "# cyto_iso   = resample_isotropic(cyto_signal, spacing, order=1)     # images -> order=1\n",
    "# mask_iso = resample_isotropic(masked_region, spacing, order=0)   # labels -> order=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = skimage.segmentation.watershed(\n",
    "#         image=cyto_signal,\n",
    "#         markers=nuclei_mask,\n",
    "#         mask=masked_region,\n",
    "#         connectivity=0, # 1\n",
    "#         compactness=0, # 0\n",
    "#     )\n",
    "# z = 12\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.subplot(131)\n",
    "# plt.imshow(nuclei_mask[z], cmap=\"nipy_spectral\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Nuclei Mask Dilated\")\n",
    "# plt.subplot(132)\n",
    "# plt.imshow(cyto_signal[z], cmap=\"inferno\")\n",
    "# # plt.title(cyto_signal_name)\n",
    "# plt.axis(\"off\")\n",
    "# plt.subplot(133)\n",
    "# plt.imshow(labels[z], cmap=\"nipy_spectral\")\n",
    "# plt.title(\"cell mask\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c45c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(np.unique(nuclei_mask)))\n",
    "# print(len(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = pathlib.Path(f\"{mask_path}/cell_mask_new_watershed.tiff\")\n",
    "# tifffile.imwrite(save_path, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea4314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = sequential_labeling(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_df = get_labels_for_post_hoc_reassignment(\n",
    "#     compartment_mask=labels, compartment_name=\"cell\"\n",
    "# )\n",
    "# nuclei_df = get_labels_for_post_hoc_reassignment(\n",
    "#     compartment_mask=nuclei_mask, compartment_name=\"nuclei\"\n",
    "# )\n",
    "# nuclei_mask, reassigned_nuclei_df = run_post_hoc_mask_reassignment(\n",
    "#     nuclei_mask=nuclei_mask,\n",
    "#     cell_mask=cell_mask,\n",
    "#     nuclei_df=nuclei_df,\n",
    "#     cell_df=cell_df,\n",
    "#     return_dataframe=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c8a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = np.array(labels)\n",
    "# labels = run_post_hoc_refinement(\n",
    "#     mask_image=labels,\n",
    "#     sliding_window_context=3,\n",
    "# )\n",
    "# cell_mask_output = pathlib.Path(f\"{mask_path}/cell_mask_watershed_posthoc.tiff\")\n",
    "# tifffile.imwrite(cell_mask_output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155202a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6170d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # butter\n",
    "# # sobel\n",
    "# # gauss\n",
    "# # butter - sobel\n",
    "# # butter - gauss\n",
    "# # sobel - butter\n",
    "# # sobel gauss\n",
    "# # gauss - sobel\n",
    "# # gauss - butter\n",
    "# # butter - sobel - gauss\n",
    "# # butter - gauss - sobel\n",
    "# # sobel - gauss - butter\n",
    "# # sobel - butter - gauss\n",
    "# # gauss - butter - sobel\n",
    "# # gauss - sobel - butter\n",
    "# butter = skimage.filters.butterworth(cyto2)\n",
    "# sobel = skimage.filters.sobel(cyto2)\n",
    "# gauss = skimage.filters.gaussian(cyto2)\n",
    "# butter_sobel = skimage.filters.butterworth(sobel)\n",
    "# butter_gauss = skimage.filters.butterworth(gauss)\n",
    "# sobel_butter = skimage.filters.sobel(butter)\n",
    "# sobel_gauss = skimage.filters.sobel(gauss)\n",
    "# gauss_sobel = skimage.filters.gaussian(sobel)\n",
    "# gauss_butter = skimage.filters.gaussian(butter)\n",
    "# butter_sobel_gauss = skimage.filters.butterworth(sobel_gauss)\n",
    "# butter_gauss_sobel = skimage.filters.butterworth(gauss_sobel)\n",
    "# sobel_gauss_butter = skimage.filters.sobel(gauss_butter)\n",
    "# sobel_butter_gauss = skimage.filters.sobel(butter_gauss)\n",
    "# gauss_butter_sobel = skimage.filters.gaussian(butter_sobel)\n",
    "# gauss_sobel_butter = skimage.filters.gaussian(sobel_butter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cytosignals = {\n",
    "#     \"butter\": butter,\n",
    "#     \"sobel\": sobel,\n",
    "#     \"gauss\": gauss,\n",
    "#     \"butter_sobel\": butter_sobel,\n",
    "#     \"butter_gauss\": butter_gauss,\n",
    "#     \"sobel_butter\": sobel_butter,\n",
    "#     \"sobel_gauss\": sobel_gauss,\n",
    "#     \"gauss_sobel\": gauss_sobel,\n",
    "#     \"gauss_butter\": gauss_butter,\n",
    "#     \"butter_sobel_gauss\": butter_sobel_gauss,\n",
    "#     \"butter_gauss_sobel\": butter_gauss_sobel,\n",
    "#     \"sobel_gauss_butter\": sobel_gauss_butter,\n",
    "#     \"sobel_butter_gauss\": sobel_butter_gauss,\n",
    "#     \"gauss_butter_sobel\": gauss_butter_sobel,\n",
    "#     \"gauss_sobel_butter\": gauss_sobel_butter,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937f2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cyto_signal_name, cyto_signal in cytosignals.items():\n",
    "#     cell_mask = segment_cells_with_3D_watershed(\n",
    "#         cyto_signal=cyto_signal,\n",
    "#         nuclei_mask=nuclei_mask_dilated,\n",
    "#         masked_region=cyto2_threshold_signal\n",
    "#     )\n",
    "#     plt.subplot(131)\n",
    "#     plt.imshow(nuclei_mask_dilated[8])\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(\"Nuclei Mask Dilated\")\n",
    "#     plt.subplot(132)\n",
    "#     plt.imshow(cyto_signal[8], cmap=\"inferno\")\n",
    "#     plt.title(cyto_signal_name)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.subplot(133)\n",
    "#     plt.imshow(cell_mask[8], cmap=\"nipy_spectral\")\n",
    "#     plt.title(\"cell mask\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaab3e5",
   "metadata": {},
   "source": [
    "## Fill holes on a per label basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e09427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop through each z-slice and fill holes for each label individually\n",
    "\n",
    "# # loop through each z-slice\n",
    "# for z in range(cell_mask.shape[0]):\n",
    "#     # copy this slice\n",
    "#     cell_mask_slice = cell_mask[z,:,:].copy()\n",
    "#     # get the labels present in this slice\n",
    "#     labels = np.unique(cell_mask_slice)\n",
    "#     # loop through each label and fill holes\n",
    "#     slice_images_to_max_proj = []\n",
    "#     for label in labels:\n",
    "#         # copt the slice so we can isolate the label\n",
    "#         cell_mask_slice_single_label = cell_mask_slice.copy()\n",
    "#         # isolate the label\n",
    "#         cell_mask_slice_single_label[cell_mask_slice_single_label != label] = 0\n",
    "#         # fill the holes\n",
    "#         cell_mask_slice_single_label = scipy.ndimage.binary_fill_holes(cell_mask_slice_single_label)\n",
    "#         cell_mask_cleaned = skimage.morphology.binary_closing(\n",
    "#             cell_mask_slice_single_label > 0,  # Convert to binary\n",
    "#             skimage.morphology.disk(10)  # Adjust radius to speckle size\n",
    "#         )\n",
    "#         # remove small objects\n",
    "#         cell_mask_cleaned = skimage.morphology.remove_small_objects(\n",
    "#             cell_mask_cleaned,\n",
    "#             min_size=100,  # Adjust size threshold as needed\n",
    "#         )\n",
    "#         # remove large objects\n",
    "#         labeled_temp = skimage.measure.label(cell_mask_cleaned)\n",
    "#         props = skimage.measure.regionprops(labeled_temp)\n",
    "#         for prop in props:\n",
    "#             if prop.area > 250000: # 500 x 500 pixel artifacts (50umx50um)  # Adjust threshold\n",
    "#                 cell_mask_cleaned[labeled_temp == prop.label] = 0\n",
    "#             cell_mask_cleaned = cell_mask_cleaned.astype(np.uint16)\n",
    "#         # reassign the label such that it is the original label and not binary\n",
    "#         cell_mask_cleaned[cell_mask_cleaned > 0] = label\n",
    "#         slice_images_to_max_proj.append(cell_mask_cleaned)\n",
    "#     # max project the slice images to get the final filled slice\n",
    "#     cell_mask_slice_max_proj = np.max(np.array(slice_images_to_max_proj), axis=0)\n",
    "#     # overwrite the original slice with the filled slice\n",
    "#     cell_mask[z,:,:] = cell_mask_slice_max_proj\n",
    "# for z in range(cell_mask.shape[0]):\n",
    "#     # z=cyto2.shape[0] // 2\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.title(\"cell_mask\")\n",
    "#     plt.imshow(cell_mask[z], cmap=cmap_option)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917e790",
   "metadata": {},
   "source": [
    "## Cell SAM for cell segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from cellSAM import cellsam_pipeline, get_model\n",
    "from cellSAM.utils import format_image_shape, normalize_image\n",
    "\n",
    "# show the access token is set\n",
    "# print(\"DEEPCELL_ACCESS_TOKEN set:\", \"DEEPCELL_ACCESS_TOKEN\" in os.environ)\n",
    "# print(os.environ[\"DEEPCELL_ACCESS_TOKEN\"])\n",
    "# get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca93c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_mask = cyto2.copy() * 0\n",
    "for z in range(cyto2.shape[0]):\n",
    "    cyto2_slice = cyto2[z, :, :].copy()\n",
    "    mask = cellsam_pipeline(\n",
    "        cyto2_slice,\n",
    "        use_wsi=False,\n",
    "        low_contrast_enhancement=False,\n",
    "        gauge_cell_size=False,\n",
    "    ).astype(np.uint16)\n",
    "    cell_mask[z, :, :] = mask\n",
    "cell_mask = cell_mask.astype(np.uint16)\n",
    "\n",
    "cell_mask, _, _ = skimage.segmentation.relabel_sequential(cell_mask)\n",
    "# plt.imshow(cell_mask[8], cmap=\"nipy_spectral\")\n",
    "# plt.title(\"Relabeled Masks\")\n",
    "# plt.show()\n",
    "if in_notebook:\n",
    "    # for z in range(cell_mask.shape[0]):\n",
    "    z = cyto2.shape[0] // 2\n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(nuclei_mask[z, :, :], cmap=\"nipy_spectral\")\n",
    "    plt.title(f\"Nuclei Mask {z}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(cyto2[z, :, :], cmap=\"inferno\")\n",
    "    plt.title(f\"Cyto2 Slice {z}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(cell_mask[z, :, :], cmap=\"nipy_spectral\")\n",
    "    plt.title(f\"CellSAM Segmentation Mask {z}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38664f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segment3D.file_io as uSegment3D_fio\n",
    "import segment3D.filters as uSegment3D_filters\n",
    "import segment3D.flows as uSegment3D_flows\n",
    "import segment3D.parameters as uSegment3D_params  # this is useful to call default parameters, and keep track of parameter changes and for saving parameters.\n",
    "import segment3D.plotting as uSegment3D_plotting\n",
    "import segment3D.segmentation as uSegment3D_segment\n",
    "import segment3D.usegment3d as uSegment3D\n",
    "import segment3D.watershed as uSegment3D_watershed\n",
    "\n",
    "\n",
    "def orthogonal_views(\n",
    "    image: np.ndarray,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate the 2D slice labels.\n",
    "    - this transposes the input  volume into xy, xz, yz stacks, then goes slice-by-slice and ensures every ID corresponds to one spatial connected component.\n",
    "    \"\"\"\n",
    "    # xy\n",
    "    labels_xy = image.copy()\n",
    "    labels_xy = uSegment3D_filters.filter_2d_label_slices(\n",
    "        labels_xy, bg_label=0, minsize=8\n",
    "    )\n",
    "\n",
    "    # xz\n",
    "    labels_xz = image.transpose(1, 0, 2).copy()\n",
    "    labels_xz = uSegment3D_filters.filter_2d_label_slices(\n",
    "        labels_xz, bg_label=0, minsize=8\n",
    "    )\n",
    "\n",
    "    # zy\n",
    "    labels_zy = image.transpose(0, 2, 1).copy()\n",
    "    labels_zy = uSegment3D_filters.filter_2d_label_slices(\n",
    "        labels_zy, bg_label=0, minsize=8\n",
    "    )\n",
    "\n",
    "    # yz\n",
    "    labels_yz = image.transpose(2, 0, 1).copy()\n",
    "    labels_yz = uSegment3D_filters.filter_2d_label_slices(\n",
    "        labels_yz, bg_label=0, minsize=8\n",
    "    )\n",
    "    # Plot orthogonal views (XY, XZ, YZ) with correct axis orientation, scaling, and matching widths\n",
    "    voxel_size_xy = 0.1  # replace with pixel size in XY (e.g., um/pixel)\n",
    "    voxel_size_z = 1.0  # replace with z-step size (e.g., um)\n",
    "\n",
    "    Z, Y, X = image.shape\n",
    "    z_mid, y_mid, x_mid = Z // 2, Y // 2, X // 2\n",
    "\n",
    "    xy = labels_xy[z_mid]  # (Y, X)\n",
    "    xz = labels_xz[y_mid]  # (Z, X)\n",
    "    yz = labels_yz[x_mid]  # (Z, Y)\n",
    "    yz_rot = np.rot90(yz)  # rotate 90Â° to get (Y, Z)\n",
    "\n",
    "    max_xy = max(X, Y) * voxel_size_xy\n",
    "    max_z = Z * voxel_size_z\n",
    "\n",
    "    extent_xy = [0, max_xy, max_xy, 0]\n",
    "    extent_xz = [0, max_xy, max_z, 0]  # force X span to match XY width\n",
    "    extent_zy = [0, max_z, max_xy, 0]\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Manually position axes to eliminate gaps - adjusted bottom position of top plot\n",
    "    ax_xz = fig.add_axes([0.05, 0.50, 0.45, 0.45])  # changed from 0.52 to 0.50\n",
    "    ax_xz.imshow(\n",
    "        xz, cmap=\"nipy_spectral\", origin=\"upper\", extent=extent_xz, aspect=\"equal\"\n",
    "    )\n",
    "    ax_xz.axis(\"off\")\n",
    "\n",
    "    ax_xy = fig.add_axes([0.05, 0.05, 0.45, 0.45])\n",
    "    ax_xy.imshow(\n",
    "        xy, cmap=\"nipy_spectral\", origin=\"upper\", extent=extent_xy, aspect=\"equal\"\n",
    "    )\n",
    "    ax_xy.axis(\"off\")\n",
    "\n",
    "    ax_yz = fig.add_axes([0.52, 0.05, 0.45, 0.45])\n",
    "    ax_yz.imshow(\n",
    "        yz_rot, cmap=\"nipy_spectral\", origin=\"upper\", extent=extent_zy, aspect=\"equal\"\n",
    "    )\n",
    "    ax_yz.axis(\"off\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd31949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def build_complete_bipartite_graph(masks_2d, distance_threshold=None):\n",
    "    \"\"\"\n",
    "    Build a complete bipartite graph from 2D segmentation masks.\n",
    "\n",
    "    For each pair of consecutive slices, connect EVERY object in slice N\n",
    "    to EVERY object in slice N+1, computing Euclidean distance between centroids.\n",
    "\n",
    "    Args:\n",
    "        masks_2d: List of 2D segmentation masks (numpy arrays)\n",
    "        distance_threshold: Optional maximum distance to include edges.\n",
    "                           If None, ALL pairs are connected (truly complete).\n",
    "\n",
    "    Returns:\n",
    "        G: NetworkX graph\n",
    "        df: DataFrame with all edges\n",
    "    \"\"\"\n",
    "    from scipy import ndimage\n",
    "\n",
    "    G = nx.Graph()\n",
    "    edges_data = []\n",
    "\n",
    "    # For each slice, find all objects and their centroids\n",
    "    slice_objects = {}\n",
    "\n",
    "    for z, mask in enumerate(masks_2d):\n",
    "        unique_ids = np.unique(mask)\n",
    "        unique_ids = unique_ids[unique_ids > 0]  # Remove background\n",
    "\n",
    "        if len(unique_ids) == 0:\n",
    "            slice_objects[z] = {}\n",
    "            continue\n",
    "\n",
    "        centroids = ndimage.center_of_mass(mask, labels=mask, index=unique_ids)\n",
    "\n",
    "        slice_objects[z] = {}\n",
    "        for obj_id, centroid in zip(unique_ids, centroids):\n",
    "            slice_objects[z][obj_id] = centroid\n",
    "            G.add_node(\n",
    "                f\"z{z}_id{obj_id}\", slice=z, original_label=obj_id, coordinates=centroid\n",
    "            )\n",
    "\n",
    "    # Build complete bipartite graph between consecutive slices\n",
    "    for z in range(len(masks_2d) - 1):\n",
    "        if z not in slice_objects or (z + 1) not in slice_objects:\n",
    "            continue\n",
    "\n",
    "        curr_objects = slice_objects[z]\n",
    "        next_objects = slice_objects[z + 1]\n",
    "\n",
    "        if not curr_objects or not next_objects:\n",
    "            continue\n",
    "\n",
    "        curr_ids = list(curr_objects.keys())\n",
    "        next_ids = list(next_objects.keys())\n",
    "\n",
    "        curr_centroids = np.array([curr_objects[id] for id in curr_ids])\n",
    "        next_centroids = np.array([next_objects[id] for id in next_ids])\n",
    "\n",
    "        # Compute all pairwise distances\n",
    "        distances = cdist(curr_centroids, next_centroids, metric=\"euclidean\")\n",
    "\n",
    "        # Add edges for ALL pairs (truly complete graph)\n",
    "        for i, curr_id in enumerate(curr_ids):\n",
    "            for j, next_id in enumerate(next_ids):\n",
    "                dist = distances[i, j]\n",
    "\n",
    "                # Note: distance_threshold is now NOT applied here\n",
    "                # (use greedy matching threshold instead)\n",
    "\n",
    "                curr_node = f\"z{z}_id{curr_id}\"\n",
    "                next_node = f\"z{z + 1}_id{next_id}\"\n",
    "\n",
    "                G.add_edge(\n",
    "                    curr_node,\n",
    "                    next_node,\n",
    "                    weight=dist,\n",
    "                    slice1=z,\n",
    "                    slice2=z + 1,\n",
    "                    original_label1=curr_id,\n",
    "                    original_label2=next_id,\n",
    "                )\n",
    "\n",
    "                edges_data.append(\n",
    "                    {\n",
    "                        \"index1\": curr_node,\n",
    "                        \"index2\": next_node,\n",
    "                        \"slice1\": z,\n",
    "                        \"slice2\": z + 1,\n",
    "                        \"original_label1\": curr_id,\n",
    "                        \"original_label2\": next_id,\n",
    "                        \"distance\": dist,\n",
    "                        \"coordinates1\": curr_objects[curr_id],\n",
    "                        \"coordinates2\": next_objects[next_id],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(edges_data)\n",
    "    return G, df\n",
    "\n",
    "\n",
    "def solve_graph_improved(G, max_distance=100.0, slices=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Solve bipartite matching across consecutive slices using greedy matching\n",
    "    based on edge weights (distances).\n",
    "\n",
    "    Instead of Hungarian algorithm (which can force bad matches), greedily match\n",
    "    objects in order of smallest distance. This ensures we only connect objects\n",
    "    that are genuinely close.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX graph with edges between different slices\n",
    "        max_distance: Maximum distance to accept a match.\n",
    "        slices: Optional list of slice numbers to process\n",
    "        verbose: Print debugging info\n",
    "\n",
    "    Returns:\n",
    "        List of paths, where each path is a list of node IDs\n",
    "    \"\"\"\n",
    "\n",
    "    if G.number_of_nodes() == 0:\n",
    "        return []\n",
    "\n",
    "    # Get unique slices from nodes\n",
    "    all_slices = set()\n",
    "    for node in G.nodes():\n",
    "        slice_id = G.nodes[node][\"slice\"]\n",
    "        all_slices.add(slice_id)\n",
    "\n",
    "    slices = sorted(all_slices)\n",
    "\n",
    "    if len(slices) < 2:\n",
    "        # Only one slice, each node is its own path\n",
    "        return [[node] for node in G.nodes()]\n",
    "\n",
    "    # For each slice, store its nodes\n",
    "    slice_to_nodes = defaultdict(list)\n",
    "    for node in G.nodes():\n",
    "        slice_id = G.nodes[node][\"slice\"]\n",
    "        slice_to_nodes[slice_id].append(node)\n",
    "\n",
    "    # Track assignments across slices\n",
    "    node_to_trajectory = {}\n",
    "    trajectory_id_counter = 0\n",
    "    trajectories = defaultdict(list)\n",
    "\n",
    "    # Match consecutive slices\n",
    "    for i in range(len(slices) - 1):\n",
    "        curr_slice = slices[i]\n",
    "        next_slice = slices[i + 1]\n",
    "\n",
    "        curr_nodes = sorted(slice_to_nodes[curr_slice])\n",
    "        next_nodes = sorted(slice_to_nodes[next_slice])\n",
    "\n",
    "        if not curr_nodes or not next_nodes:\n",
    "            continue\n",
    "\n",
    "        # Get all edges between these slices, sorted by weight\n",
    "        edges = []\n",
    "        for curr_node in curr_nodes:\n",
    "            for next_node in next_nodes:\n",
    "                if G.has_edge(curr_node, next_node):\n",
    "                    weight = G[curr_node][next_node][\"weight\"]\n",
    "                    edges.append((weight, curr_node, next_node))\n",
    "\n",
    "        if not edges:\n",
    "            if verbose:\n",
    "                print(f\"Warning: No edges between slice {curr_slice} and {next_slice}\")\n",
    "            continue\n",
    "\n",
    "        # Sort by distance (smallest first)\n",
    "        edges.sort(key=lambda x: x[0])\n",
    "\n",
    "        matched_curr = set()\n",
    "        matched_next = set()\n",
    "\n",
    "        # Priority 1: Match nodes that already have trajectories (prefer continuity)\n",
    "        priority_edges = []\n",
    "        new_edges = []\n",
    "\n",
    "        for dist, curr_node, next_node in edges:\n",
    "            if dist > max_distance:\n",
    "                continue\n",
    "            if curr_node in matched_curr or next_node in matched_next:\n",
    "                continue\n",
    "\n",
    "            # Prioritize edges where curr_node already has a trajectory\n",
    "            if curr_node in node_to_trajectory:\n",
    "                priority_edges.append((dist, curr_node, next_node))\n",
    "            else:\n",
    "                new_edges.append((dist, curr_node, next_node))\n",
    "\n",
    "        # Match priority edges first (continues existing trajectories)\n",
    "        for dist, curr_node, next_node in priority_edges:\n",
    "            if curr_node in matched_curr or next_node in matched_next:\n",
    "                continue\n",
    "\n",
    "            traj_id = node_to_trajectory[curr_node]\n",
    "            node_to_trajectory[next_node] = traj_id\n",
    "            trajectories[traj_id].append(next_node)\n",
    "            matched_curr.add(curr_node)\n",
    "            matched_next.add(next_node)\n",
    "\n",
    "        # Then match new edges (start new trajectories)\n",
    "        for dist, curr_node, next_node in new_edges:\n",
    "            if curr_node in matched_curr or next_node in matched_next:\n",
    "                continue\n",
    "\n",
    "            traj_id = trajectory_id_counter\n",
    "            trajectory_id_counter += 1\n",
    "            trajectories[traj_id].append(curr_node)\n",
    "            trajectories[traj_id].append(next_node)\n",
    "            node_to_trajectory[curr_node] = traj_id\n",
    "            node_to_trajectory[next_node] = traj_id\n",
    "            matched_curr.add(curr_node)\n",
    "            matched_next.add(next_node)\n",
    "\n",
    "        # Create new trajectories for unmatched next_nodes\n",
    "        for next_node in next_nodes:\n",
    "            if next_node not in matched_next and next_node not in node_to_trajectory:\n",
    "                trajectories[trajectory_id_counter] = [next_node]\n",
    "                node_to_trajectory[next_node] = trajectory_id_counter\n",
    "                trajectory_id_counter += 1\n",
    "\n",
    "    # Add any untracked nodes as single-node paths\n",
    "    for node in G.nodes():\n",
    "        if node not in node_to_trajectory:\n",
    "            trajectories[trajectory_id_counter] = [node]\n",
    "            trajectory_id_counter += 1\n",
    "\n",
    "    return list(trajectories.values())\n",
    "\n",
    "\n",
    "def split_long_trajectories(paths, max_length):\n",
    "    \"\"\"\n",
    "    Split trajectories that exceed max_length into shorter ones.\n",
    "\n",
    "    Args:\n",
    "        paths: List of node paths\n",
    "        max_length: Maximum number of consecutive nodes per trajectory\n",
    "\n",
    "    Returns:\n",
    "        List of split trajectories\n",
    "    \"\"\"\n",
    "    split_paths = []\n",
    "\n",
    "    for path in paths:\n",
    "        if len(path) <= max_length:\n",
    "            split_paths.append(path)\n",
    "        else:\n",
    "            # Split into chunks of max_length\n",
    "            for i in range(0, len(path), max_length):\n",
    "                chunk = path[i : i + max_length]\n",
    "                if chunk:\n",
    "                    split_paths.append(chunk)\n",
    "\n",
    "    return split_paths\n",
    "\n",
    "\n",
    "def collapse_labels_from_paths(masks_2d, paths):\n",
    "    \"\"\"\n",
    "    Assign unified labels based on trajectories.\n",
    "\n",
    "    Args:\n",
    "        masks_2d: List of 2D masks\n",
    "        paths: List of node paths from solve_graph_improved\n",
    "\n",
    "    Returns:\n",
    "        List of 3D masks with unified labels\n",
    "    \"\"\"\n",
    "    # Create mapping from node ID to trajectory label\n",
    "    node_to_label = {}\n",
    "    for label_id, path in enumerate(paths):\n",
    "        for node_id in path:\n",
    "            node_to_label[node_id] = label_id\n",
    "\n",
    "    # Relabel each 2D mask\n",
    "    relabeled_masks = []\n",
    "    for z, mask in enumerate(masks_2d):\n",
    "        new_mask = np.zeros_like(mask)\n",
    "\n",
    "        unique_ids = np.unique(mask)\n",
    "        unique_ids = unique_ids[unique_ids > 0]\n",
    "\n",
    "        for obj_id in unique_ids:\n",
    "            node_id = f\"z{z}_id{obj_id}\"\n",
    "            if node_id in node_to_label:\n",
    "                new_label = node_to_label[node_id]\n",
    "                new_mask[mask == obj_id] = new_label\n",
    "\n",
    "        relabeled_masks.append(new_mask)\n",
    "\n",
    "    return relabeled_masks\n",
    "\n",
    "\n",
    "def stack_3d_segmentation(relabeled_masks):\n",
    "    \"\"\"Stack 2D relabeled masks into 3D volume.\"\"\"\n",
    "    return np.stack(relabeled_masks, axis=0)\n",
    "\n",
    "\n",
    "def remove_single_slice_objects(segmentation_3d):\n",
    "    \"\"\"\n",
    "    Remove objects that only appear in a single z-slice.\n",
    "\n",
    "    Args:\n",
    "        segmentation_3d: 3D segmentation array\n",
    "\n",
    "    Returns:\n",
    "        Cleaned 3D segmentation with single-slice objects removed\n",
    "    \"\"\"\n",
    "    cleaned = segmentation_3d.copy()\n",
    "\n",
    "    # Find all unique labels\n",
    "    unique_labels = np.unique(segmentation_3d)\n",
    "    unique_labels = unique_labels[unique_labels > 0]\n",
    "\n",
    "    # For each label, count how many slices it appears in\n",
    "    for label in unique_labels:\n",
    "        slices_with_label = np.where(np.any(segmentation_3d == label, axis=(1, 2)))[0]\n",
    "\n",
    "        # If label only appears in 1 slice, remove it\n",
    "        if len(slices_with_label) == 1:\n",
    "            cleaned[segmentation_3d == label] = 0\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def fill_object_gaps(segmentation_3d, max_gap_size=2):\n",
    "    \"\"\"\n",
    "    Fill gaps in object trajectories (missing slices between appearances).\n",
    "\n",
    "    For example, if object ID 5 appears in slices [10, 11, 14, 15],\n",
    "    the gap between 11 and 14 will be filled if gap_size <= max_gap_size.\n",
    "\n",
    "    Args:\n",
    "        segmentation_3d: 3D segmentation array\n",
    "        max_gap_size: Maximum number of consecutive missing slices to fill\n",
    "                     (default: 2, meaning fill gaps of 1-2 slices)\n",
    "\n",
    "    Returns:\n",
    "        Filled 3D segmentation\n",
    "    \"\"\"\n",
    "    filled = segmentation_3d.copy()\n",
    "\n",
    "    # Find all unique labels\n",
    "    unique_labels = np.unique(segmentation_3d)\n",
    "    unique_labels = unique_labels[unique_labels > 0]\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Find all slices where this label appears\n",
    "        slices_with_label = np.where(np.any(segmentation_3d == label, axis=(1, 2)))[0]\n",
    "\n",
    "        if len(slices_with_label) < 2:\n",
    "            continue\n",
    "\n",
    "        # Check for gaps and fill them\n",
    "        for i in range(len(slices_with_label) - 1):\n",
    "            curr_slice = slices_with_label[i]\n",
    "            next_slice = slices_with_label[i + 1]\n",
    "            gap_size = next_slice - curr_slice - 1\n",
    "\n",
    "            # If gap is small enough, fill it by interpolating from neighbors\n",
    "            if 0 < gap_size <= max_gap_size:\n",
    "                # Get the mask from current and next slice\n",
    "                curr_mask = segmentation_3d[curr_slice] == label\n",
    "                next_mask = segmentation_3d[next_slice] == label\n",
    "\n",
    "                # Interpolate: use union of both masks for gap slices\n",
    "                combined_mask = curr_mask | next_mask\n",
    "\n",
    "                for gap_slice in range(curr_slice + 1, next_slice):\n",
    "                    filled[gap_slice][combined_mask] = label\n",
    "\n",
    "    return filled\n",
    "\n",
    "\n",
    "def postprocess_segmentation(\n",
    "    segmentation_3d, remove_singletons=True, fill_gaps=True, max_gap_size=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-process 3D segmentation to clean up artifacts.\n",
    "\n",
    "    Args:\n",
    "        segmentation_3d: 3D segmentation array\n",
    "        remove_singletons: If True, remove objects that only appear in 1 slice\n",
    "        fill_gaps: If True, fill small gaps in object trajectories\n",
    "        max_gap_size: Maximum gap size to fill (only used if fill_gaps=True)\n",
    "\n",
    "    Returns:\n",
    "        Cleaned 3D segmentation\n",
    "    \"\"\"\n",
    "    result = segmentation_3d.copy()\n",
    "\n",
    "    if remove_singletons:\n",
    "        result = remove_single_slice_objects(result)\n",
    "\n",
    "    if fill_gaps:\n",
    "        result = fill_object_gaps(result, max_gap_size)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def full_pipeline(\n",
    "    masks_2d, max_match_distance=100.0, max_trajectory_length=None, verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete pipeline: build complete bipartite graph -> solve matching -> relabel.\n",
    "\n",
    "    Args:\n",
    "        masks_2d: List of 2D segmentation masks\n",
    "        max_match_distance: Maximum distance to accept a match (in pixels).\n",
    "        max_trajectory_length: Optional maximum number of consecutive slices an object\n",
    "                              can span. If None, no limit. Use to prevent unrealistic\n",
    "                              tall objects (e.g., set to 10 if cells shouldn't span >10 slices).\n",
    "        verbose: Print diagnostics\n",
    "\n",
    "    Returns:\n",
    "        segmentation_3d: 3D array with unified instance labels across slices\n",
    "        diagnostics: Dict with stats about the matching\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Building complete bipartite graph...\")\n",
    "    G, df = build_complete_bipartite_graph(masks_2d)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        print(f\"\\nDistance statistics:\")\n",
    "        if len(df) > 0:\n",
    "            print(f\"  Mean distance: {df['distance'].mean():.2f}\")\n",
    "            print(f\"  Median distance: {df['distance'].median():.2f}\")\n",
    "            print(f\"  Std dev: {df['distance'].std():.2f}\")\n",
    "            print(f\"  Min: {df['distance'].min():.2f}, Max: {df['distance'].max():.2f}\")\n",
    "            print(f\"  25th percentile: {df['distance'].quantile(0.25):.2f}\")\n",
    "            print(f\"  75th percentile: {df['distance'].quantile(0.75):.2f}\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"\\nSolving bipartite matching (max_match_distance={max_match_distance})...\"\n",
    "        )\n",
    "\n",
    "    paths = solve_graph_improved(G, max_distance=max_match_distance, verbose=verbose)\n",
    "\n",
    "    # Post-process: split trajectories that are too long\n",
    "    if max_trajectory_length is not None:\n",
    "        paths = split_long_trajectories(paths, max_trajectory_length)\n",
    "\n",
    "    diagnostics = {\n",
    "        \"num_trajectories\": len(paths),\n",
    "        \"trajectory_lengths\": [len(p) for p in paths],\n",
    "        \"mean_trajectory_length\": np.mean([len(p) for p in paths]),\n",
    "        \"distance_stats\": {\n",
    "            \"mean\": df[\"distance\"].mean() if len(df) > 0 else 0,\n",
    "            \"median\": df[\"distance\"].median() if len(df) > 0 else 0,\n",
    "            \"std\": df[\"distance\"].std() if len(df) > 0 else 0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Found {len(paths)} trajectories\")\n",
    "        print(\n",
    "            f\"  Mean trajectory length: {diagnostics['mean_trajectory_length']:.2f} slices\"\n",
    "        )\n",
    "        print(f\"  Max trajectory length: {max(diagnostics['trajectory_lengths'])}\")\n",
    "        single_slice = sum(1 for p in paths if len(p) == 1)\n",
    "        print(f\"  Single-slice trajectories: {single_slice}\")\n",
    "        print(\"Relabeling masks...\")\n",
    "\n",
    "    relabeled_masks = collapse_labels_from_paths(masks_2d, paths)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Stacking into 3D volume...\")\n",
    "\n",
    "    segmentation_3d = stack_3d_segmentation(relabeled_masks)\n",
    "\n",
    "    # Post-process: remove single-slice objects and fill small gaps\n",
    "    segmentation_3d = postprocess_segmentation(\n",
    "        segmentation_3d, remove_singletons=True, fill_gaps=True, max_gap_size=2\n",
    "    )\n",
    "\n",
    "    return segmentation_3d, diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6553d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the coordinates dataframe for reconstruction\n",
    "# coordinates_df = generate_coordinates_for_reconstruction(cell_mask)\n",
    "# # generate distance pairs dataframe\n",
    "# df = generate_distance_pairs(coordinates_df, x_y_vector_radius_max_constraint=20) # might need to change the radius contraint based on data\n",
    "# generate and solve graph to get longest paths\n",
    "# longest_paths = solve_graph(graph_creation(df))\n",
    "# # collapse labels based on longest paths and reassign labels in nuclei masks\n",
    "# image = reassign_labels(cell_mask, collapse_labels(coordinates_df, longest_paths))\n",
    "# # refine the nuclei masks\n",
    "# original_methods_cell_mask = run_post_hoc_refinement(\n",
    "#     mask_image=image,\n",
    "#     sliding_window_context=3,\n",
    "# )\n",
    "\n",
    "# del image, coordinates_df, df, longest_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_3d, diag = full_pipeline(cell_mask, max_match_distance=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\n",
    "    \"distance\": [],\n",
    "    \"num_trajectories\": [],\n",
    "    \"mean_trajectory_length\": [],\n",
    "    \"mean_distance_stats\": [],\n",
    "    \"median_distance_stats\": [],\n",
    "    \"std_distance_stats\": [],\n",
    "}\n",
    "for i in tqdm.tqdm(range(0, 250, 10)):\n",
    "    _, diag = full_pipeline(cell_mask, max_match_distance=i, verbose=False)\n",
    "    results_dict[\"distance\"].append(i)\n",
    "    results_dict[\"num_trajectories\"].append(diag[\"num_trajectories\"])\n",
    "    results_dict[\"mean_trajectory_length\"].append(diag[\"mean_trajectory_length\"])\n",
    "    results_dict[\"mean_distance_stats\"].append(\n",
    "        np.median(diag[\"distance_stats\"][\"mean\"])\n",
    "    )\n",
    "    results_dict[\"median_distance_stats\"].append(\n",
    "        np.median(diag[\"distance_stats\"][\"median\"])\n",
    "    )\n",
    "    results_dict[\"std_distance_stats\"].append(np.median(diag[\"distance_stats\"][\"std\"]))\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(221)\n",
    "plt.plot(results_df[\"distance\"], results_df[\"num_trajectories\"], marker=\"o\")\n",
    "plt.title(\"Number of Trajectories vs. Max Match Distance\")\n",
    "plt.xlabel(\"Max Match Distance\")\n",
    "plt.ylabel(\"Number of Trajectories\")\n",
    "plt.subplot(222)\n",
    "plt.plot(\n",
    "    results_df[\"distance\"],\n",
    "    results_df[\"mean_trajectory_length\"],\n",
    "    marker=\"o\",\n",
    "    color=\"orange\",\n",
    ")\n",
    "plt.title(\"Mean Trajectory Length vs. Max Match Distance\")\n",
    "plt.xlabel(\"Max Match Distance\")\n",
    "plt.ylabel(\"Mean Trajectory Length\")\n",
    "plt.subplot(223)\n",
    "# get the dirivative of num_trajectories with respect to distance\n",
    "derivative_num_trajectories = np.gradient(\n",
    "    results_df[\"num_trajectories\"], results_df[\"distance\"]\n",
    ")\n",
    "plt.plot(results_df[\"distance\"], derivative_num_trajectories, marker=\"o\", color=\"green\")\n",
    "plt.title(\"Derivative of Number of Trajectories vs. Max Match Distance\")\n",
    "plt.xlabel(\"Max Match Distance\")\n",
    "plt.ylabel(\"d(Number of Trajectories)/d(Distance)\")\n",
    "plt.subplot(224)\n",
    "# get the derivative of mean_trajectory_length with respect to distance\n",
    "derivative_mean_trajectory_length = np.gradient(\n",
    "    results_df[\"mean_trajectory_length\"], results_df[\"distance\"]\n",
    ")\n",
    "plt.plot(\n",
    "    results_df[\"distance\"], derivative_mean_trajectory_length, marker=\"o\", color=\"red\"\n",
    ")\n",
    "plt.title(\"Derivative of Mean Trajectory Length vs. Max Match Distance\")\n",
    "plt.xlabel(\"Max Match Distance\")\n",
    "plt.ylabel(\"d(Mean Trajectory Length)/d(Distance)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc009e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_3d, diag = full_pipeline(\n",
    "    cell_mask, max_match_distance=75, max_trajectory_length=12, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347522d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "orthogonal_views(segmentation_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5de188",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_mask_output = pathlib.Path(f\"{mask_path}/cell_mask_stitched_graph.tiff\")\n",
    "tifffile.imwrite(cell_mask_output, segmentation_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_3d_post_hoc = run_post_hoc_refinement(\n",
    "    mask_image=segmentation_3d,\n",
    "    sliding_window_context=3,\n",
    ")\n",
    "cell_mask_output = pathlib.Path(f\"{mask_path}/cell_mask_stitched_post_hoc.tiff\")\n",
    "tifffile.imwrite(cell_mask_output, segmentation_3d_post_hoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06000c59",
   "metadata": {},
   "source": [
    "## Save segs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f251b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "relabeled_cells = np.array(segmentation_3d_post_hoc)\n",
    "segmentation_3d_post_hoc = run_post_hoc_refinement(\n",
    "    mask_image=relabeled_cells,\n",
    "    sliding_window_context=3,\n",
    ")\n",
    "cell_mask_output = pathlib.Path(f\"{mask_path}/cell_mask_post_hoc_post_stitching.tiff\")\n",
    "tifffile.imwrite(cell_mask_output, segmentation_3d_post_hoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b4209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890560c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e740743",
   "metadata": {},
   "source": [
    "## run the mask reassignment function (post-hoc)\n",
    "### This needs to occur after both nuclei and cell segmentations are done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741523e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_df = get_labels_for_post_hoc_reassignment(\n",
    "#     compartment_mask=cell_mask, compartment_name=\"cell\"\n",
    "# )\n",
    "# nuclei_df = get_labels_for_post_hoc_reassignment(\n",
    "#     compartment_mask=nuclei_mask, compartment_name=\"nuclei\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92661ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclei_mask, reassigned_nuclei_df = run_post_hoc_mask_reassignment(\n",
    "#     nuclei_mask=nuclei_mask,\n",
    "#     cell_mask=cell_mask,\n",
    "#     nuclei_df=nuclei_df,\n",
    "#     cell_df=cell_df,\n",
    "#     return_dataframe=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8cd55",
   "metadata": {},
   "source": [
    "## Cytoplasm Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cytoplasm_mask = create_cytoplasm_masks(\n",
    "#     nuclei_masks=nuclei_mask,\n",
    "#     cell_masks=cell_mask,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e14f56",
   "metadata": {},
   "source": [
    "## Organoid segmentation (derived from cell segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bb2f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the cell masks to binary masks\n",
    "# cell_binary_mask = cell_mask.copy()\n",
    "# cell_binary_mask[cell_binary_mask > 0] = 1\n",
    "# # dilate the cell masks slightly\n",
    "# cell_binary_mask = skimage.morphology.binary_dilation(\n",
    "#     cell_binary_mask, skimage.morphology.ball(10)\n",
    "# )\n",
    "# # convert back to instance mask\n",
    "# # make sure each instance has a unique integer label\n",
    "# organoid_mask = skimage.measure.label(cell_binary_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8abe9",
   "metadata": {},
   "source": [
    "## Save the segmented masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ac29b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclei_mask_output = pathlib.Path(f\"{mask_path}/nuclei_mask.tiff\")\n",
    "cell_mask_output = pathlib.Path(f\"{mask_path}/cell_mask.tiff\")\n",
    "# cytoplasm_mask_output = pathlib.Path(f\"{mask_path}/cytoplasm_mask.tiff\")\n",
    "# organoid_mask_output = pathlib.Path(f\"{mask_path}/organoid_mask.tiff\")\n",
    "# tifffile.imwrite(nuclei_mask_output, nuclei_mask)\n",
    "tifffile.imwrite(cell_mask_output, cell_mask)\n",
    "# tifffile.imwrite(cytoplasm_mask_output, cytoplasm_mask)\n",
    "# # tifffile.imwrite(organoid_mask_output, organoid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92baf2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_mem = psutil.Process(os.getpid()).memory_info().rss / 1024**2\n",
    "# end_time = time.time()\n",
    "# print(f\"\"\"\n",
    "#     Memory and time profiling for the run:\\n\n",
    "#     Memory usage: {end_mem - start_mem:.2f} MB\\n\n",
    "#     Time:\\n\n",
    "#     --- %s seconds --- % {(end_time - start_time)}\\n\n",
    "#     --- %s minutes --- % {((end_time - start_time) / 60)}\\n\n",
    "#     --- %s hours --- % {((end_time - start_time) / 3600)}\n",
    "# \"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GFF_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
